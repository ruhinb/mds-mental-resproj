{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "734d728f",
   "metadata": {},
   "source": [
    "## Data Scrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4f6c942d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîé Scraping r/depression...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jm/36rrr53s1w388ms7208drrcm0000gn/T/ipykernel_21099/4174835129.py:58: DeprecationWarning: datetime.datetime.utcfromtimestamp() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.fromtimestamp(timestamp, datetime.UTC).\n",
      "  'created_utc': datetime.utcfromtimestamp(post.created_utc).strftime('%Y-%m-%d %H:%M:%S'),\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 87\u001b[39m\n\u001b[32m     84\u001b[39m             comment_data.append(comment_entry)\n\u001b[32m     86\u001b[39m         \u001b[38;5;66;03m# Optional: be nice to Reddit API\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m87\u001b[39m         time.sleep(\u001b[32m1\u001b[39m)\n\u001b[32m     89\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     90\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m‚ö†Ô∏è Error scraping r/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msub\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import praw\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import time\n",
    "import os\n",
    "\n",
    "# --- SETUP REDDIT AUTH ---\n",
    "reddit = praw.Reddit(\n",
    "    client_id='trvEAgsIJjIc4SpF1kASCg',\n",
    "    client_secret='QRgVLo1_3h1B02PINYv5uUcc1hK9aA',\n",
    "    user_agent='mental-health-resproj',\n",
    "    username='kohor_',\n",
    "    password='Sq5dS$*T7#8%gua92o@9'\n",
    ")\n",
    "\n",
    "# --- CONFIG ---\n",
    "subreddits = [\n",
    "    'depression',\n",
    "    'depression_help',\n",
    "    'anxiety',\n",
    "    'mentalhealth',\n",
    "    'SuicideWatch',\n",
    "    'DecidingToBeBetter',\n",
    "    'BipolarReddit',\n",
    "    'OCD',\n",
    "    'ADHD',\n",
    "    'therapy',\n",
    "    'StopSelfHarm',\n",
    "    'selfhelp',\n",
    "    'socialanxiety'\n",
    "]\n",
    "\n",
    "start_year = 2015\n",
    "start_timestamp = int(datetime(start_year, 1, 1).timestamp())\n",
    "\n",
    "post_data = []\n",
    "comment_data = []\n",
    "\n",
    "# --- SCRAPE ---\n",
    "for sub in subreddits:\n",
    "    print(f\"üîé Scraping r/{sub}...\")\n",
    "    try:\n",
    "        for post in reddit.subreddit(sub).new(limit=None):  # Get as many as allowed\n",
    "            if post.created_utc < start_timestamp:\n",
    "                continue\n",
    "\n",
    "            post_author = post.author.name if post.author else None\n",
    "\n",
    "            # Store post\n",
    "            post_data.append({\n",
    "                'subreddit': sub,\n",
    "                'id': post.id,\n",
    "                'title': post.title,\n",
    "                'text': post.selftext,\n",
    "                'author': post_author,\n",
    "                'score': post.score,\n",
    "                'num_comments': post.num_comments,\n",
    "                'created_utc': datetime.utcfromtimestamp(post.created_utc).strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                'url': post.url\n",
    "            })\n",
    "\n",
    "            # Load comments\n",
    "            post.comments.replace_more(limit=0)\n",
    "            for top_comment in post.comments:\n",
    "                comment_author = top_comment.author.name if top_comment.author else None\n",
    "                comment_entry = {\n",
    "                    'post_id': post.id,\n",
    "                    'comment_id': top_comment.id,\n",
    "                    'comment_author': comment_author,\n",
    "                    'comment_body': top_comment.body,\n",
    "                    'comment_score': top_comment.score,\n",
    "                    'comment_created': datetime.utcfromtimestamp(top_comment.created_utc).strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                    'author_reply': None,\n",
    "                    'author_reply_id': None\n",
    "                }\n",
    "\n",
    "                # Search for author reply to this comment\n",
    "                for reply in top_comment.replies:\n",
    "                    if reply.author and reply.author.name == post_author:\n",
    "                        comment_entry['author_reply'] = reply.body\n",
    "                        comment_entry['author_reply_id'] = reply.id\n",
    "                        break  # First author reply is enough\n",
    "\n",
    "                comment_data.append(comment_entry)\n",
    "\n",
    "            # Optional: be nice to Reddit API\n",
    "            time.sleep(1)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error scraping r/{sub}: {e}\")\n",
    "        continue\n",
    "\n",
    "# --- SAVE ---\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "\n",
    "# Save posts\n",
    "df_posts = pd.DataFrame(post_data)\n",
    "df_posts.to_csv(\"data/reddit_posts_mentalhealth_2015_present.csv\", index=False)\n",
    "\n",
    "# Save comments + replies\n",
    "df_comments = pd.DataFrame(comment_data)\n",
    "df_comments.to_csv(\"data/reddit_comments_with_author_replies.csv\", index=False)\n",
    "\n",
    "print(f\"\\n‚úÖ Scraped {len(df_posts)} posts and {len(df_comments)} comments with potential author replies.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b97c2fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîé Scraping r/depression (up to 2000 posts)...\n",
      "‚ö†Ô∏è Error scraping r/depression: received 401 HTTP response\n",
      "üîé Scraping r/depression_help (up to 2000 posts)...\n",
      "‚ö†Ô∏è Error scraping r/depression_help: received 401 HTTP response\n",
      "üîé Scraping r/anxiety (up to 2000 posts)...\n",
      "‚ö†Ô∏è Error scraping r/anxiety: received 401 HTTP response\n",
      "üîé Scraping r/mentalhealth (up to 2000 posts)...\n",
      "‚ö†Ô∏è Error scraping r/mentalhealth: received 401 HTTP response\n",
      "üîé Scraping r/SuicideWatch (up to 2000 posts)...\n",
      "‚ö†Ô∏è Error scraping r/SuicideWatch: received 401 HTTP response\n",
      "üîé Scraping r/DecidingToBeBetter (up to 2000 posts)...\n",
      "‚ö†Ô∏è Error scraping r/DecidingToBeBetter: received 401 HTTP response\n",
      "üîé Scraping r/BipolarReddit (up to 2000 posts)...\n",
      "‚ö†Ô∏è Error scraping r/BipolarReddit: received 401 HTTP response\n",
      "üîé Scraping r/OCD (up to 2000 posts)...\n",
      "‚ö†Ô∏è Error scraping r/OCD: received 401 HTTP response\n",
      "üîé Scraping r/ADHD (up to 2000 posts)...\n",
      "‚ö†Ô∏è Error scraping r/ADHD: received 401 HTTP response\n",
      "üîé Scraping r/therapy (up to 2000 posts)...\n",
      "‚ö†Ô∏è Error scraping r/therapy: received 401 HTTP response\n",
      "üîé Scraping r/StopSelfHarm (up to 2000 posts)...\n",
      "‚ö†Ô∏è Error scraping r/StopSelfHarm: received 401 HTTP response\n",
      "üîé Scraping r/selfhelp (up to 2000 posts)...\n",
      "‚ö†Ô∏è Error scraping r/selfhelp: received 401 HTTP response\n",
      "üîé Scraping r/socialanxiety (up to 2000 posts)...\n",
      "‚ö†Ô∏è Error scraping r/socialanxiety: received 401 HTTP response\n",
      "\n",
      "‚úÖ Done! 0 posts and 0 comments (with author replies where available) saved.\n"
     ]
    }
   ],
   "source": [
    "import praw\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "# --- SETUP REDDIT AUTH ---\n",
    "reddit = praw.Reddit(\n",
    "    client_id='trvEAgsIJjIc4SpF1kASCg',\n",
    "    client_secret='QRgVLo1_3h1B02PINYv5uUcc1kK9aA',\n",
    "    user_agent='mental-health-resproj',\n",
    "    username='kohor_',\n",
    "    password='Sq5dS$*T7#8%gua92o@9'\n",
    ")\n",
    "\n",
    "# --- CONFIG ---\n",
    "subreddits = [\n",
    "    'depression',\n",
    "    'depression_help',\n",
    "    'anxiety',\n",
    "    'mentalhealth',\n",
    "    'SuicideWatch',\n",
    "    'DecidingToBeBetter',\n",
    "    'BipolarReddit',\n",
    "    'OCD',\n",
    "    'ADHD',\n",
    "    'therapy',\n",
    "    'StopSelfHarm',\n",
    "    'selfhelp',\n",
    "    'socialanxiety'\n",
    "]\n",
    "\n",
    "post_limit = 2000\n",
    "\n",
    "post_data = []\n",
    "comment_data = []\n",
    "\n",
    "# --- SCRAPE ---\n",
    "for sub in subreddits:\n",
    "    print(f\"üîé Scraping r/{sub} (up to {post_limit} posts)...\")\n",
    "    try:\n",
    "        for post in reddit.subreddit(sub).new(limit=post_limit):\n",
    "            post_author = post.author.name if post.author else None\n",
    "\n",
    "            # Store post info\n",
    "            post_data.append({\n",
    "                'subreddit': sub,\n",
    "                'post_id': post.id,\n",
    "                'title': post.title,\n",
    "                'text': post.selftext,\n",
    "                'author': post_author,\n",
    "                'score': post.score,\n",
    "                'num_comments': post.num_comments,\n",
    "                'created_utc': datetime.utcfromtimestamp(post.created_utc).strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                'url': post.url\n",
    "            })\n",
    "\n",
    "            # Load comments\n",
    "            post.comments.replace_more(limit=0)\n",
    "            for top_comment in post.comments:\n",
    "                comment_author = top_comment.author.name if top_comment.author else None\n",
    "                comment_entry = {\n",
    "                    'post_id': post.id,\n",
    "                    'comment_id': top_comment.id,\n",
    "                    'comment_author': comment_author,\n",
    "                    'comment_body': top_comment.body,\n",
    "                    'comment_score': top_comment.score,\n",
    "                    'comment_created': datetime.utcfromtimestamp(top_comment.created_utc).strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                    'author_reply': None,\n",
    "                    'author_reply_id': None\n",
    "                }\n",
    "\n",
    "                # Check if post author replied to this comment\n",
    "                for reply in top_comment.replies:\n",
    "                    if reply.author and reply.author.name == post_author:\n",
    "                        comment_entry['author_reply'] = reply.body\n",
    "                        comment_entry['author_reply_id'] = reply.id\n",
    "                        break  # only first author reply\n",
    "\n",
    "                comment_data.append(comment_entry)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error scraping r/{sub}: {e}\")\n",
    "        continue\n",
    "\n",
    "# --- SAVE TO CSV ---\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "\n",
    "df_posts = pd.DataFrame(post_data)\n",
    "df_posts.to_csv(\"data/reddit_data_posts.csv\", index=False)\n",
    "\n",
    "df_comments = pd.DataFrame(comment_data)\n",
    "df_comments.to_csv(\"data/reddit_data_comments_with_author_replies.csv\", index=False)\n",
    "\n",
    "print(f\"\\n‚úÖ Done! {len(df_posts)} posts and {len(df_comments)} comments (with author replies where available) saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "37a0dcfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîé Scraping r/depression (target: 2000 posts)...\n",
      "‚ùå Error scraping r/depression: received 401 HTTP response\n",
      "\n",
      "üîé Scraping r/depression_help (target: 2000 posts)...\n",
      "‚ùå Error scraping r/depression_help: received 401 HTTP response\n",
      "\n",
      "üîé Scraping r/anxiety (target: 2000 posts)...\n",
      "‚ùå Error scraping r/anxiety: received 401 HTTP response\n",
      "\n",
      "üîé Scraping r/mentalhealth (target: 2000 posts)...\n",
      "‚ùå Error scraping r/mentalhealth: received 401 HTTP response\n",
      "\n",
      "üîé Scraping r/SuicideWatch (target: 2000 posts)...\n",
      "‚ùå Error scraping r/SuicideWatch: received 401 HTTP response\n",
      "\n",
      "üîé Scraping r/DecidingToBeBetter (target: 2000 posts)...\n",
      "‚ùå Error scraping r/DecidingToBeBetter: received 401 HTTP response\n",
      "\n",
      "üîé Scraping r/BipolarReddit (target: 2000 posts)...\n",
      "‚ùå Error scraping r/BipolarReddit: received 401 HTTP response\n",
      "\n",
      "üîé Scraping r/OCD (target: 2000 posts)...\n",
      "‚ùå Error scraping r/OCD: received 401 HTTP response\n",
      "\n",
      "üîé Scraping r/ADHD (target: 2000 posts)...\n",
      "‚ùå Error scraping r/ADHD: received 401 HTTP response\n",
      "\n",
      "üîé Scraping r/therapy (target: 2000 posts)...\n",
      "‚ùå Error scraping r/therapy: received 401 HTTP response\n",
      "\n",
      "üîé Scraping r/StopSelfHarm (target: 2000 posts)...\n",
      "‚ùå Error scraping r/StopSelfHarm: received 401 HTTP response\n",
      "\n",
      "üîé Scraping r/selfhelp (target: 2000 posts)...\n",
      "‚ùå Error scraping r/selfhelp: received 401 HTTP response\n",
      "\n",
      "üîé Scraping r/socialanxiety (target: 2000 posts)...\n",
      "‚ùå Error scraping r/socialanxiety: received 401 HTTP response\n",
      "\n",
      "‚úÖ Done! 0 posts and 0 comments saved.\n"
     ]
    }
   ],
   "source": [
    "import praw\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import os\n",
    "import time\n",
    "\n",
    "# --- SETUP REDDIT AUTH ---\n",
    "reddit = praw.Reddit(\n",
    "    client_id='trvEAgsIJjIc4SpF1kASCg',\n",
    "    client_secret='QRgVLo1_3h1B02PINYv5uUcc1kK9aA',\n",
    "    user_agent='mental-health-resproj',\n",
    "    username='kohor_',\n",
    "    password='Sq5dS$*T7#8%gua92o@9'\n",
    ")\n",
    "\n",
    "# --- CONFIG ---\n",
    "subreddits = [\n",
    "    'depression', 'depression_help', 'anxiety', 'mentalhealth',\n",
    "    'SuicideWatch', 'DecidingToBeBetter', 'BipolarReddit',\n",
    "    'OCD', 'ADHD', 'therapy', 'StopSelfHarm', 'selfhelp', 'socialanxiety'\n",
    "]\n",
    "\n",
    "target_limit = 2000  # total posts per subreddit\n",
    "chunk_size = 200     # posts per request\n",
    "sleep_secs = 2       # delay between chunks\n",
    "\n",
    "post_data = []\n",
    "comment_data = []\n",
    "\n",
    "for sub in subreddits:\n",
    "    print(f\"\\nüîé Scraping r/{sub} (target: {target_limit} posts)...\")\n",
    "    count = 0\n",
    "    try:\n",
    "        submissions = reddit.subreddit(sub).new(limit=None)\n",
    "        for post in submissions:\n",
    "            if count >= target_limit:\n",
    "                break\n",
    "\n",
    "            post_author = post.author.name if post.author else None\n",
    "            post_data.append({\n",
    "                'subreddit': sub,\n",
    "                'post_id': post.id,\n",
    "                'title': post.title,\n",
    "                'text': post.selftext,\n",
    "                'author': post_author,\n",
    "                'score': post.score,\n",
    "                'num_comments': post.num_comments,\n",
    "                'created_utc': datetime.utcfromtimestamp(post.created_utc).strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                'url': post.url\n",
    "            })\n",
    "\n",
    "            try:\n",
    "                post.comments.replace_more(limit=0)\n",
    "                for top_comment in post.comments:\n",
    "                    comment_author = top_comment.author.name if top_comment.author else None\n",
    "                    comment_entry = {\n",
    "                        'post_id': post.id,\n",
    "                        'comment_id': top_comment.id,\n",
    "                        'comment_author': comment_author,\n",
    "                        'comment_body': top_comment.body,\n",
    "                        'comment_score': top_comment.score,\n",
    "                        'comment_created': datetime.utcfromtimestamp(top_comment.created_utc).strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                        'author_reply': None,\n",
    "                        'author_reply_id': None\n",
    "                    }\n",
    "\n",
    "                    for reply in top_comment.replies:\n",
    "                        if reply.author and reply.author.name == post_author:\n",
    "                            comment_entry['author_reply'] = reply.body\n",
    "                            comment_entry['author_reply_id'] = reply.id\n",
    "                            break\n",
    "\n",
    "                    comment_data.append(comment_entry)\n",
    "            except Exception as ce:\n",
    "                print(f\"‚ö†Ô∏è Error loading comments for post {post.id}: {ce}\")\n",
    "\n",
    "            count += 1\n",
    "\n",
    "            if count % chunk_size == 0:\n",
    "                print(f\"  ...scraped {count} posts from r/{sub} so far\")\n",
    "                time.sleep(sleep_secs)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error scraping r/{sub}: {e}\")\n",
    "        continue\n",
    "\n",
    "# --- SAVE TO CSV ---\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "df_posts = pd.DataFrame(post_data)\n",
    "df_posts.to_csv(\"data/reddit_data_posts.csv\", index=False)\n",
    "\n",
    "df_comments = pd.DataFrame(comment_data)\n",
    "df_comments.to_csv(\"data/reddit_data_comments_with_author_replies.csv\", index=False)\n",
    "\n",
    "print(f\"\\n‚úÖ Done! {len(df_posts)} posts and {len(df_comments)} comments saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d105bf48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîé Scraping top posts from r/depression...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jm/36rrr53s1w388ms7208drrcm0000gn/T/ipykernel_21099/2715727796.py:50: DeprecationWarning: datetime.datetime.utcfromtimestamp() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.fromtimestamp(timestamp, datetime.UTC).\n",
      "  'created_utc': datetime.utcfromtimestamp(post.created_utc).strftime('%Y-%m-%d %H:%M:%S'),\n",
      "/var/folders/jm/36rrr53s1w388ms7208drrcm0000gn/T/ipykernel_21099/2715727796.py:65: DeprecationWarning: datetime.datetime.utcfromtimestamp() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.fromtimestamp(timestamp, datetime.UTC).\n",
      "  'comment_created': datetime.utcfromtimestamp(top_comment.created_utc).strftime('%Y-%m-%d %H:%M:%S'),\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 80\u001b[39m\n\u001b[32m     77\u001b[39m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ce:\n\u001b[32m     78\u001b[39m             \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m‚ö†Ô∏è Comment error in post \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpost.id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mce\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m80\u001b[39m         time.sleep(\u001b[32m0.5\u001b[39m)  \u001b[38;5;66;03m# polite delay\u001b[39;00m\n\u001b[32m     82\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     83\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m‚ùå Error scraping r/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msub\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import praw\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import time\n",
    "import os\n",
    "\n",
    "# --- SETUP REDDIT AUTH ---\n",
    "reddit = praw.Reddit(\n",
    "    client_id='trvEAgsIJjIc4SpF1kASCg',\n",
    "    client_secret='QRgVLo1_3h1B02PINYv5uUcc1hK9aA',\n",
    "    user_agent='mental-health-resproj',\n",
    "    username='kohor_',\n",
    "    password='Sq5dS$*T7#8%gua92o@9'\n",
    ")\n",
    "\n",
    "# --- CONFIG ---\n",
    "subreddits = [\n",
    "    'depression', 'depression_help', 'anxiety', 'mentalhealth',\n",
    "    'SuicideWatch', 'DecidingToBeBetter', 'BipolarReddit',\n",
    "    'OCD', 'ADHD', 'therapy', 'StopSelfHarm', 'selfhelp', 'socialanxiety'\n",
    "]\n",
    "\n",
    "start_year = 2015\n",
    "start_timestamp = int(datetime(start_year, 1, 1).timestamp())\n",
    "\n",
    "post_limit = 500  # sensible number per subreddit\n",
    "\n",
    "post_data = []\n",
    "comment_data = []\n",
    "\n",
    "# --- SCRAPE ---\n",
    "for sub in subreddits:\n",
    "    print(f\"üîé Scraping top posts from r/{sub}...\")\n",
    "    try:\n",
    "        for post in reddit.subreddit(sub).top(limit=post_limit):\n",
    "            if post.created_utc < start_timestamp:\n",
    "                continue  # skip old posts\n",
    "\n",
    "            post_author = post.author.name if post.author else None\n",
    "\n",
    "            # Save post\n",
    "            post_data.append({\n",
    "                'subreddit': sub,\n",
    "                'id': post.id,\n",
    "                'title': post.title,\n",
    "                'text': post.selftext,\n",
    "                'author': post_author,\n",
    "                'score': post.score,\n",
    "                'num_comments': post.num_comments,\n",
    "                'created_utc': datetime.utcfromtimestamp(post.created_utc).strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                'url': post.url\n",
    "            })\n",
    "\n",
    "            # Comments and author replies\n",
    "            try:\n",
    "                post.comments.replace_more(limit=0)\n",
    "                for top_comment in post.comments:\n",
    "                    comment_author = top_comment.author.name if top_comment.author else None\n",
    "                    comment_entry = {\n",
    "                        'post_id': post.id,\n",
    "                        'comment_id': top_comment.id,\n",
    "                        'comment_author': comment_author,\n",
    "                        'comment_body': top_comment.body,\n",
    "                        'comment_score': top_comment.score,\n",
    "                        'comment_created': datetime.utcfromtimestamp(top_comment.created_utc).strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                        'author_reply': None,\n",
    "                        'author_reply_id': None\n",
    "                    }\n",
    "\n",
    "                    for reply in top_comment.replies:\n",
    "                        if reply.author and reply.author.name == post_author:\n",
    "                            comment_entry['author_reply'] = reply.body\n",
    "                            comment_entry['author_reply_id'] = reply.id\n",
    "                            break\n",
    "\n",
    "                    comment_data.append(comment_entry)\n",
    "            except Exception as ce:\n",
    "                print(f\"‚ö†Ô∏è Comment error in post {post.id}: {ce}\")\n",
    "\n",
    "            time.sleep(0.5)  # polite delay\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error scraping r/{sub}: {e}\")\n",
    "        continue\n",
    "\n",
    "# --- SAVE ---\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "\n",
    "df_posts = pd.DataFrame(post_data)\n",
    "df_posts.to_csv(\"data/reddit_posts_top500.csv\", index=False)\n",
    "\n",
    "df_comments = pd.DataFrame(comment_data)\n",
    "df_comments.to_csv(\"data/reddit_comments_top500.csv\", index=False)\n",
    "\n",
    "print(f\"\\n‚úÖ Finished! {len(df_posts)} posts and {len(df_comments)} comments saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8fcb0035",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîé Scraping top 500 posts from r/depression...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jm/36rrr53s1w388ms7208drrcm0000gn/T/ipykernel_21099/2459199829.py:38: DeprecationWarning: datetime.datetime.utcfromtimestamp() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.fromtimestamp(timestamp, datetime.UTC).\n",
      "  'created_utc': datetime.utcfromtimestamp(post.created_utc).strftime('%Y-%m-%d %H:%M:%S'),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîé Scraping top 500 posts from r/depression_help...\n",
      "üîé Scraping top 500 posts from r/anxiety...\n",
      "üîé Scraping top 500 posts from r/mentalhealth...\n",
      "üîé Scraping top 500 posts from r/SuicideWatch...\n",
      "üîé Scraping top 500 posts from r/DecidingToBeBetter...\n",
      "üîé Scraping top 500 posts from r/BipolarReddit...\n",
      "üîé Scraping top 500 posts from r/OCD...\n",
      "üîé Scraping top 500 posts from r/ADHD...\n",
      "üîé Scraping top 500 posts from r/therapy...\n",
      "üîé Scraping top 500 posts from r/StopSelfHarm...\n",
      "‚ùå Error scraping r/StopSelfHarm: received 403 HTTP response\n",
      "üîé Scraping top 500 posts from r/selfhelp...\n",
      "üîé Scraping top 500 posts from r/socialanxiety...\n",
      "\n",
      "‚úÖ Done! Scraped 6000 posts. Saved to: data/reddit_top500_posts_per_subreddit.csv\n"
     ]
    }
   ],
   "source": [
    "import praw\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "# --- SETUP REDDIT AUTH ---\n",
    "reddit = praw.Reddit(\n",
    "    client_id='trvEAgsIJjIc4SpF1kASCg',\n",
    "    client_secret='QRgVLo1_3h1B02PINYv5uUcc1hK9aA',\n",
    "    user_agent='mental-health-resproj',\n",
    "    username='kohor_',\n",
    "    password='Sq5dS$*T7#8%gua92o@9'\n",
    ")\n",
    "\n",
    "# --- CONFIG ---\n",
    "subreddits = [\n",
    "    'depression', 'depression_help', 'anxiety', 'mentalhealth',\n",
    "    'SuicideWatch', 'DecidingToBeBetter', 'BipolarReddit',\n",
    "    'OCD', 'ADHD', 'therapy', 'StopSelfHarm', 'selfhelp', 'socialanxiety'\n",
    "]\n",
    "\n",
    "post_limit = 500  # number of top posts per subreddit\n",
    "post_data = []\n",
    "\n",
    "# --- SCRAPE POSTS ONLY ---\n",
    "for sub in subreddits:\n",
    "    print(f\"üîé Scraping top {post_limit} posts from r/{sub}...\")\n",
    "    try:\n",
    "        for post in reddit.subreddit(sub).top(limit=post_limit):\n",
    "            post_data.append({\n",
    "                'subreddit': sub,\n",
    "                'id': post.id,\n",
    "                'title': post.title,\n",
    "                'text': post.selftext,\n",
    "                'author': post.author.name if post.author else None,\n",
    "                'score': post.score,\n",
    "                'num_comments': post.num_comments,\n",
    "                'created_utc': datetime.utcfromtimestamp(post.created_utc).strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                'url': post.url\n",
    "            })\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error scraping r/{sub}: {e}\")\n",
    "        continue\n",
    "\n",
    "# --- SAVE ---\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "df = pd.DataFrame(post_data)\n",
    "df.to_csv(\"data/reddit_top500_posts_per_subreddit.csv\", index=False)\n",
    "\n",
    "print(f\"\\n‚úÖ Done! Scraped {len(df)} posts. Saved to: data/reddit_top500_posts_per_subreddit.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "resproj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
